// comment
Now one specific thing for the preprocessing in question answering is how to deal with very
 long documents. We usually truncate them in other tasks, when they are longer than the
 model maximum sentence length, but here, removing part of the the context might
 result in losing the answer we are looking for. To deal with this, we will allow one (long)
 example in our dataset to give several input features, each of length shorter than the
 maximum length of the model (or the one we set as a hyper-parameter). 
Also, just in case the answer lies at the point we split a long context, we allow some
overlap between the features we generate controlled by the hyper-parameter doc_stride:
[CLS] question tokens [SEP] context tokens [SEP]
// command
return_overflowing_tokens=True 
     # split the data into overlapped parts and create input features.
// command
offset_mapping  
    # The offset mappings will give us a map from token to character position in the original context. This will
    # help us compute the start_positions and end_positions.
    # Looks like [[(0,0),(0,3),(3,4)...] ] - Contains the actual start indices and end indices for each word in the input.
// command
sequence_ids
It returns None for the special tokens, then 0 or 1 depending on whether the corresponding token comes from the
first sentence past (the question) or the second (the context).
